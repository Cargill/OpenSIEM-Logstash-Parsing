input {
  pipeline { address => s3_output }
}
output {
  s3 {
    region => "us-east-1"
    bucket => "VAR_BUCKET_NAME" # bucket name is different in dev and prod
    codec => "json_lines"
    server_side_encryption => true # encryption at rest (AES)
    # increase to 32 from 4 as this would be shared accross
    upload_queue_size => 128 # number is for files not logs so should be good
    size_file => 5242880 # 5mb default
    time_file => 15 # 15 minutes default
    rotation_strategy => "size_and_time" # first one to match rotates the file
    prefix => "%{[@metadata][index]}/%{+xxxx/MM/dd}" # folder name, config name would be the folder name
    encoding => gzip # compress the 15 minutes or 5 mb logs before sending
    upload_workers_count => 128 # increase to 64 from 8 as this would be shared accross
    restore => false # maybe this solves corrupt files issue in s3
  }
}