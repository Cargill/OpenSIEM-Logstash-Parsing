# Copyright [2021] [Cargill, Incorporated.] 
# SPDX-License-Identifier: Apache-2.0
input {
  pipeline {
    address => VAR_PIPELINE_NAME
  }
}
filter {
  mutate {
    add_field => { "event.module" => "sdwan.application" }
    add_field => { "observer.vendor" => "versa" }
    add_field => { "observer.product" => "versa.sdwan.application" }
    add_field => { "observer.type" => "network_management" }
    }
    ### Header
  ### Logstash tcp or udp in
  if [agent.type] == "logstash" {
    dissect {
      tag_on_failure => "_dateparsefailure_header"
        mapping => {
            message => "<%{pri}> %{rest_msg}"
        }
    }
  } else {
  ### Filebeats udp or tcp in
    dissect {
      tag_on_failure => "_dateparsefailure_header"
        mapping => {
            message => "%{?data} {%{?data}} <%{pri}> %{rest_msg}"
        }
    }
  }
  syslog_pri {
    syslog_pri_field_name => "pri"
  }
  mutate{
    remove_field => ["pri"]
  }
    # 1. Start by copying the original message to log.original.  We'll then pick the common fields and placing them under [tmp]. [tmp] is a temporary slot to work out the parsing

    mutate {
        add_field => {"[log.original]" => "%{[message]}"}
    }

    dissect {
        mapping => {
        "message" => "%{} %{[tmp][host]} %{[tmp][time]} %{[tmp][dataset]}, %{[tmp][rest_msg]}"
        }
    }

    # a. add the observer.product value to dataset
    mutate {
      #add_field => { "event.dataset" => "%{[observer.product]}.%{[tmp][dataset]}"}
      rename => { "[tmp][dataset]" => "event.dataset"}
    }

    # 2. Convert the timestamp and place that into [tmp][time]
    date {
        match => [ "[tmp][time]", "MMM dd yyyy HH:mm:ss",
              "MMM  d yyyy HH:mm:ss", "ISO8601" ]
    }

    # 3. Extract the host provided in the beggining of the message. Move it to observer
    grok {
        match => { "[tmp][host]" => "{name=%{GREEDYDATA:observer.hostname}}" }
    }


    # 4. Take the rest of the SDWAN message, which is comprised of field=value split by comma. Throw in [tmp][rest_msg]
    
    kv {
        source => "[tmp][rest_msg]"
        field_split => "="
        field_split_pattern => ", "
        target => "[tmp][rest_msg]"
       # whitespace => "strict" #check later
    }

    # 5. We end up with all SDWAN specific fields from rest msg. Now use 2 mutate filters (for readability) to perform the actions a. and b. below
    # Please note that Versa has several different types of logs. These types of logs are the different datasets that were parsed in step 1.a. not all fields will show up as results of the kv on step 4,
    # therefore not all fields will find a match to be renamed. this is expected and helps using a single filter to address different types of logs

        #a) Parse fields that belong to ECS into ECS fields
    mutate {
        
        # common
        rename => { "[tmp][rest_msg][generateTime]" => "event.created"}
        rename => { "[tmp][rest_msg][applianceName]" => "host.hostname"}
        # when dataset is eventLog the fields below will show up
        rename => { "[tmp][rest_msg][alarmSeverity]" => "log.syslog.severity.name" }
        rename => { "[tmp][rest_msg][eventType]" => "event.action" }
        rename => { "[tmp][rest_msg][rule]" => "rule.category" }
        # when dataset is alarmlog the fields below will show up
        rename => { "[tmp][rest_msg][alarmSeverity]" => "log.syslog.severity.name" }
        rename => { "[tmp][rest_msg][alarmType]" => "event.action" }
        rename => { "[tmp][rest_msg][alarmCause]" => "event.reason"} 
        rename => { "[tmp][rest_msg][alarmSeqNo]" => "error.id"}
        rename => { "[tmp][rest_msg][alarmEventType]" => "error.type"}
        rename => { "[tmp][rest_msg][alarmText]" => "error.message"}
        rename => { "[tmp][rest_msg][alarmClass]" => "rule.category"}
        rename => { "[tmp][rest_msg][serialNum]" => "host.id"}
        rename => { "[tmp][rest_msg][siteName]" => "network.name"}

        # when dataset is idpLog the fields below will show up
        rename => { "[tmp][rest_msg][ipsProfileRule]" => "rule.name"}
        rename => { "[tmp][rest_msg][signatureMsg]" => "rule.description"}
        rename => { "[tmp][rest_msg][threatType]" => "rule.category"}
        rename => { "[tmp][rest_msg][ipsProfile]" => "rule.ruleset"}
        rename => { "[tmp][rest_msg][ipsProtocol]" => "network.protocol"}
        rename => { "[tmp][rest_msg][ipsApplication]" => "network.application"}
        rename => { "[tmp][rest_msg][ipsDirection]" => "network.direction"}
        rename => { "[tmp][rest_msg][classMsg]" => "error.message"}
        

       
    }

    # b) Parse/convert common fields
    date {
        match => [ "event.created", "UNIX" ]
        target => "event.created"
    }

    if [tmp][rest_msg][remoteSiteName] {
      mutate {
        add_field => { "related.hosts" => "%{[tmp][rest_msg][remoteSiteName]}"} 
      }
    }

    if [tmp][rest_msg][remoteSiteName] {
      mutate {
        add_field => { "related.hosts" => "%{[tmp][rest_msg][localSiteName]}"} 
      }
    }

 

    # c) Parse fields that don't belong to ECS into Labels (as per https://www.elastic.co/guide/en/ecs/current/ecs-custom-fields-in-ecs.html#_the_labels_field)
          

    

    # # 6. Proceed to hardcoded evaluations

    # event kind (alert, event, metric, state, pipeline_error, signal)
    translate {
        field => "event.dataset"
        destination => "[event.kind]"
        dictionary => {
          "alarmLog" => "alert"
          "accessLog" => "state"
          "sfwAccessLog" => "state"
          "denyLog" => "state"
          "idpLog" => "alert"
          "avLog" => "alert"
          "ipfLog" => "alert"
          "urlfLog" => "alert"
          "dosThreatLog" => "alert"
          "flowIdLog" => "state"
          "flowMonLog" => "state"
          "flowMonHttpLog" => "state"
          "monStatsLog" => "metric"
          "bwMonLog" => "metric"
          "intfUtilLog" => "metric"
          "sdwanB2BSlamLog" => "metric"
          "tcpAppMonLog" => "metric"
          "sdwanAccCktCosLog" => "metric"
        }
        fallback => "event"
    }

    # event category
    translate {
        field => "[error.type]"
        destination => "[event.category]"
        dictionary => {
          "equipmentAlarm" => "host"
        }
    }

    # event type
    translate {
        field => "[event.action]"
        destination => "[event.type]"
        dictionary => {
          "interface-down" => "error"
          "interface-up" => "info"
        }
    }

    # event outcome
    translate {
        field => "[event.dataset]"
        destination => "[event.outcome]"
        dictionary => {
          "outOfService" => "failure"
        }
    }

    # 7. Specific event handling

    # the block below seems redundant but this is the structure to parse out idp events
    if "event.dataset" == "idpLog" {
      translate {
        field => "[tmp][rest_msg][idpAction]"
        destination => "[tmp][idp_output]"
        dictionary => {
          "alert" => "alert"
        }
        fallback => "alert"
      }
      translate {
        field => "network.direction"
        destination => "network.direction"
        dictionary => {
          "ToServer" => "ingress"
        }
        fallback => "unknown"
      }
      
    }

    




    # 8. Convert fields (i.e. extract site, appliance type, etc)
    if "" in [network.name] or ![network.name]  {
        mutate {
            add_field => {"[network.name]" => "%{[host.hostname]}" }
        }
    }


    mutate {
        add_field => {"[service.type]" => "%{[host.hostname]}"}
    }


    mutate {
        gsub => [
            "[service.type]", "(.*)-([a-z]*)", "\2",
            "[network.name]", "([a-z]*)([0-9].*)", "\1"
        ]
    }
    mutate{
      remove_field => ["tmp", "rest_msg", "event_kind"]
    }
}
output {
  pipeline { send_to => [enrichments] }
}
