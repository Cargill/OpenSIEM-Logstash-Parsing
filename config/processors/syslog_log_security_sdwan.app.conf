# Copyright [2021] [Cargill, Incorporated.] 
# SPDX-License-Identifier: Apache-2.0
input {
  pipeline {
    address => VAR_PIPELINE_NAME
  }
}

filter {
  mutate {
    add_field => { "event.module" => "sdwan.application" }
    add_field => { "observer.vendor" => "versa" }
    add_field => { "observer.product" => "versa.sdwan.application" }
    add_field => { "observer.type" => "network_management" }
  }
    
  # 1. Start by copying the original message to log.original.  We'll then pick the common fields and placing them under [tmp]. [tmp] is a temporary slot to work out the parsing

  mutate {
    add_field => {"[log.original]" => "%{[message]}"}
  }

  dissect {
    mapping => {
    "message" => "%{} %{[tmp][host]} %{[tmp][time]} %{[tmp][dataset]}, %{[tmp][rest_msg]}"
    }
  }

  # 2. Convert the timestamp and place that into [tmp][time]
  date {
    match => [ "[tmp][time]", "MMM dd yyyy HH:mm:ss",
    "MMM  d yyyy HH:mm:ss", "ISO8601" ]
  }

  # 3. Extract the host prodvided in the beggining of the message. Move it to observer.hostname
  grok {
    match => { "[tmp][host]" => "{name=%{GREEDYDATA:observer.hostname}}" }
  }

  # 4. Parse the SDWAN module that generated the logs
  mutate {
    add_field => { "[event.dataset]" => "%{event.module}.%{[tmp][dataset]}"}
  }

  # 5. Take the rest of the SDWAN message, which is comprised of field=value split by comma. Throw in [tmp][rest_msg]
  kv {
    source => "[tmp][rest_msg]"
    field_split => "="
    field_split_pattern => ", "
    target => "[tmp][rest_msg]"
    # whitespace => "strict" #check later
  }

  # 6. Event categorization

  # a. event kind (alert, event, metric, state, pipeline_error, signal)
  translate {
    field => "[tmp][dataset]"
    destination => "[event.kind]"
    dictionary => {
    "alarmLog" => "alert"
    "accessLog" => "state"
    "sfwAccessLog" => "state"
    "denyLog" => "state"
    "idpLog" => "alert"
    "avLog" => "alert"
    "ipfLog" => "alert"
    "urlfLog" => "alert"
    "dosThreatLog" => "alert"
    "flowIdLog" => "state"
    "flowMonLog" => "state"
    "flowMonHttpLog" => "state"
    "monStatsLog" => "metric"
    "bwMonLog" => "metric"
    "intfUtilLog" => "metric"
    "sdwanB2BSlamLog" => "metric"
    "tcpAppMonLog" => "metric"
  }
    fallback => "event"
  }

  # 7. Each event.dataset will require its own parsing since the contents of [tmp][rest_msg] will be specific to that dataset.

    # a. Alarm Logs

    if [tmp][dataset] == "alarmLog" {
      mutate {
        rename => { "[tmp][rest_msg][applianceName]" => "host.hostname" }
        rename => { "[tmp][rest_msg][tenantName]" => "organization.name"}
        rename => { "[tmp][rest_msg][alarmType]" => "error.type"}
        rename => { "[tmp][rest_msg][alarmCause]" => "event.action"}
        rename => { "[tmp][rest_msg][alarmSeverity]" => "log.syslog.severity.name"}
        rename => { "[tmp][rest_msg][alarmSeqNo]" => "event.id"}
        rename => { "[tmp][rest_msg][alarmText]" => "error.message"}
        rename => { "[tmp][rest_msg][serialNum]" => "host.id"}
      }
      date {
        match => [ "[tmp][rest_msg][generateTime]", "UNIX" ]
        target => "event.start"
      }
    }

    # b. Access Logs
    # ...
    # q. Tcp App Mon Logs

  # 8. Convert fields (i.e. extract site, appliance type, etc)
  if "" in [network.name] or ![network.name]  {
    mutate {
        add_field => {"[network.name]" => "%{[host.hostname]}" }
    }
  }

  mutate {
      add_field => {"[service.type]" => "%{[host.hostname]}"}
  }


  mutate {
      gsub => [
          "[service.type]", "(.*)-([a-z]*)", "\2",
          "[network.name]", "([a-z]*)([0-9].*)", "\1"
      ]
  }

  # 9. Drop unwanted fields
  mutate{
    remove_field => ["rest_msg", "event_kind", "tmp"]
  }
  
}

output {
  pipeline { send_to => [enrichments] }
}
